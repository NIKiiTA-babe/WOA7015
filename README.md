# WOA7015

## Medical VQA Baseline (BLIP Fine-tuning)
Project Overview
This repository implements a baseline Medical Visual Question Answering (VQA) system by fine-tuning the BLIP (Bootstrapping Language-Image Pre-training) model. The project focuses on the VQA-RAD dataset, leveraging a pre-trained Encoder-Decoder architecture to generate clinical answers from medical images (X-ray, CT, MRI). It serves as a performance benchmark for cross-modal medical reasoning before the introduction of advanced domain-specific expert modules.
